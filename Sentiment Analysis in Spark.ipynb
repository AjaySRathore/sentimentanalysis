{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 19:30:55 WARN Utils: Your hostname, mecha resolves to a loopback address: 127.0.1.1; using 192.168.1.161 instead (on interface wlp8s0)\n",
      "23/01/19 19:30:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 19:30:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#import modules\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
    "\n",
    "#create Spark session\n",
    "appName = \"Sentiment Analysis in Spark\"\n",
    "conf = (SparkConf().setAppName(\"appName\"))\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data file into Spark dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|Target|ID        |Date                        |Query   |User           |Text                                                                                                               |\n",
      "+------+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|0     |1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "|0     |1467810672|Mon Apr 06 22:19:49 PDT 2009|NO_QUERY|scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |\n",
      "|0     |1467810917|Mon Apr 06 22:19:53 PDT 2009|NO_QUERY|mattycus       |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |\n",
      "+------+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#read csv file into dataFrame with automatically inferred schema\n",
    "schema = StructType([\n",
    "    StructField(\"Target\", IntegerType(), True),\n",
    "    StructField(\"ID\", IntegerType(), True),\n",
    "    StructField(\"Date\", StringType(), True),\n",
    "    StructField(\"Query\", StringType(), True),\n",
    "    StructField(\"User\", StringType(), True),\n",
    "    StructField(\"Text\", StringType(), True)])\n",
    "\n",
    "tweets_csv = spark.read.csv('dataset/tweets.csv', schema=schema, header=False)\n",
    "tweets_csv.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|Text                                                                                                               |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|0    |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |0    |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |0    |\n",
      "|my whole body feels itchy and like its on fire                                                                     |0    |\n",
      "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.     |0    |\n",
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select only \"SentimentText\" and \"Sentiment\" column, \n",
    "#and cast \"Sentiment\" column data into integer\n",
    "data = tweets_csv.select(\"Text\", col(\"Target\").alias(\"label\"))\n",
    "data.show(truncate = False,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim White Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|Text                                                                                                               |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|0    |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |0    |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |0    |\n",
      "|my whole body feels itchy and like its on fire                                                                     |0    |\n",
      "|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.     |0    |\n",
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "data = data.withColumn(\"Text\", trim(data.Text))\n",
    "data.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (4 + 4) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1119940 ; Testing data rows: 480060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.7, 0.3]) \n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate \"SentimentText\" into individual words using tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Text                                                                                                                       |label|SentimentWords                                                                                                                                     |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|!! @JordanisCreativ ...nice seeing you out! too bad we didn't get to chat!                                                 |0    |[!!, @jordaniscreativ, ...nice, seeing, you, out!, too, bad, we, didn't, get, to, chat!]                                                           |\n",
      "|!! De no creer     http://bit.ly/ln2Da                                                                                     |0    |[!!, de, no, creer, , , , , http://bit.ly/ln2da]                                                                                                   |\n",
      "|!!!  Awwee damnnn. Sorry Joshyy, I wish I could, but I have to work like all day... Lunch and Supper shift. @joshalexanderr|0    |[!!!, , awwee, damnnn., sorry, joshyy,, i, wish, i, could,, but, i, have, to, work, like, all, day..., lunch, and, supper, shift., @joshalexanderr]|\n",
      "|!!!  I left my teddy bear at Kim &amp; Scott's.                                                                            |0    |[!!!, , i, left, my, teddy, bear, at, kim, &amp;, scott's.]                                                                                        |\n",
      "|!!! WTF at you!! what kinda question is that to ask anybody!!! smeggy jerkoff                                              |0    |[!!!, wtf, at, you!!, what, kinda, question, is, that, to, ask, anybody!!!, smeggy, jerkoff]                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Text\", outputCol=\"SentimentWords\")\n",
    "tokenizedTrain = tokenizer.transform(trainingData)\n",
    "tokenizedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words (unimportant words to be features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|Text                                                                                                                       |label|SentimentWords                                                                                                                                     |MeaningfulWords                                                                                                  |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|!! @JordanisCreativ ...nice seeing you out! too bad we didn't get to chat!                                                 |0    |[!!, @jordaniscreativ, ...nice, seeing, you, out!, too, bad, we, didn't, get, to, chat!]                                                           |[!!, @jordaniscreativ, ...nice, seeing, out!, bad, get, chat!]                                                   |\n",
      "|!! De no creer     http://bit.ly/ln2Da                                                                                     |0    |[!!, de, no, creer, , , , , http://bit.ly/ln2da]                                                                                                   |[!!, de, creer, , , , , http://bit.ly/ln2da]                                                                     |\n",
      "|!!!  Awwee damnnn. Sorry Joshyy, I wish I could, but I have to work like all day... Lunch and Supper shift. @joshalexanderr|0    |[!!!, , awwee, damnnn., sorry, joshyy,, i, wish, i, could,, but, i, have, to, work, like, all, day..., lunch, and, supper, shift., @joshalexanderr]|[!!!, , awwee, damnnn., sorry, joshyy,, wish, could,, work, like, day..., lunch, supper, shift., @joshalexanderr]|\n",
      "|!!!  I left my teddy bear at Kim &amp; Scott's.                                                                            |0    |[!!!, , i, left, my, teddy, bear, at, kim, &amp;, scott's.]                                                                                        |[!!!, , left, teddy, bear, kim, &amp;, scott's.]                                                                 |\n",
      "|!!! WTF at you!! what kinda question is that to ask anybody!!! smeggy jerkoff                                              |0    |[!!!, wtf, at, you!!, what, kinda, question, is, that, to, ask, anybody!!!, smeggy, jerkoff]                                                       |[!!!, wtf, you!!, kinda, question, ask, anybody!!!, smeggy, jerkoff]                                             |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting words feature into numerical feature. In Spark 2.2.1,it is implemented in HashingTF funtion using Austin Appleby's MurmurHash 3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|MeaningfulWords                                                                                                  |features                                                                                                                                                                     |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[!!, @jordaniscreativ, ...nice, seeing, out!, bad, get, chat!]                                                   |(262144,[22593,82593,93729,130329,145380,153094,244891,252722],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                            |\n",
      "|0    |[!!, de, creer, , , , , http://bit.ly/ln2da]                                                                     |(262144,[43265,93729,162985,181897,249180],[1.0,1.0,1.0,1.0,4.0])                                                                                                            |\n",
      "|0    |[!!!, , awwee, damnnn., sorry, joshyy,, wish, could,, work, like, day..., lunch, supper, shift., @joshalexanderr]|(262144,[20719,28686,34343,61470,127231,144961,153642,154273,180047,208258,211525,216927,247548,249180,261724],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "    'label', 'MeaningfulWords', 'features')\n",
    "numericTrainData.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our classifier model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 19:31:37 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 19:31:59 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/01/19 19:31:59 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "model = lr.fit(numericTrainData)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 19:32:51 WARN TaskSetManager: Stage 37 contains a task of very large size (10455 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "basePath = \"/home/mecha/Documents/ml_models/sentiment_analyzer\"\n",
    "model.save(basePath + \"/modeliter11Demo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|Label|MeaningfulWords                                                                                                |features                                                                                                              |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[!@#$, tomorrow's, monday]                                                                                     |(262144,[194004,222966,229020],[1.0,1.0,1.0])                                                                         |\n",
      "|0    |[#, nyc, celebrity, street, vendors, &gt;, #, poa, celebrities, , http://streetvendor.org/media/pdfs/side2.pdf]|(262144,[28450,44254,46032,70292,114168,114353,132773,181561,247863,249180],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0])|\n",
      "+-----+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(testingData)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Label', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict testing data and calculate the accuracy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 19:32:55 WARN DAGScheduler: Broadcasting large task binary with size 10.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------+----------+-----+\n",
      "|MeaningfulWords                                                                                                |prediction|Label|\n",
      "+---------------------------------------------------------------------------------------------------------------+----------+-----+\n",
      "|[!@#$, tomorrow's, monday]                                                                                     |0.0       |0    |\n",
      "|[#, nyc, celebrity, street, vendors, &gt;, #, poa, celebrities, , http://streetvendor.org/media/pdfs/side2.pdf]|4.0       |0    |\n",
      "|[#3breakupwords, still, love, dumb, ass!]                                                                      |0.0       |0    |\n",
      "|[#3turnoffwords, &quot;scary, movie, 5&quot;]                                                                  |4.0       |0    |\n",
      "+---------------------------------------------------------------------------------------------------------------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "23/01/19 19:32:56 WARN DAGScheduler: Broadcasting large task binary with size 10.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 348192 , total data: 480060 , accuracy: 0.7253093363329584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=4, truncate = False)\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
